{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphengenerierung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist die Implementierung des Papers \"GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models\"\n",
    "Ziel ist es, ein neuronales Netzwerk zu implementieren, dass vorher mit Graphen gefüttert wird und dann neue erzeugt.\n",
    "Eine Beispielanwendung wäre, organische Moleküle als Graphen aufzufassen und dann neue potenzielle Verbindungen\n",
    "zu erstellen (You et al. 2018) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein ungerichteter Graph $G = (V, E)$ ist definiert durch seine Knotenmenge\n",
    "$V = \\{v_1 , \\dots , v_n \\} $ und die Kantenmenge $E \\subset \\{(v_i , v_j ) \\mid v_i , v_j \\in V \\} $.\n",
    "Weiterhin wollen wir eine Knotenordnung $  \\pi \\colon V \\to \\mathbb{N} $ definieren, die jedem Knoten eine natürliche Zahl zuordnet und eine injektive Funktion darstellt.\n",
    "Mithilfe der Knotenordnung können wir jedem Graphen $G$ durch eine Adjazenzmatrix $ A^\\pi \\in \\mathbb{R}^{n \\times n} $ darstellen, wobei der $ (k, l) $ - Eintrag gleich $ 1 $ ist, falls wenn $ \\pi(v_i) = k $ und  $ \\pi(v_j) = l $, dann $ (v_i, v_j) \\in E $ gilt, und sonst ist der Eintrag $ 0 $. Sei $ \\Pi $ die Menge aller Knotenordnungen des Graphen $ G $.\n",
    "\n",
    "Das Ziel des Lernens generativer Modelle von Graphen ist das Lernen\n",
    "eine Verteilung $p_{\\text{model}} (\\tilde{G}) $ über Graphen, basierend auf einer Menge von\n",
    "beobachteten Graphen $\\tilde{G} = \\{G_1 , \\dots, G_s \\} $, die aus der umbekannten Verteilung $p$ kommen, wobei jeder Graph $ G_i $ eine unterschiedliche, endliche\n",
    "Anzahl von Knoten und Kanten hat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout, to_agraph\n",
    "from queue import Queue\n",
    "from random import choice, randint\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from copy import copy\n",
    "#from data import *\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Erstellt die Daten, die wir zum Trainieren, Validieren und Testen benutzen\n",
    "## Quelle: https://github.com/snap-stanford/GraphRNN/blob/master/create_graphs.py\n",
    "def create(args):\n",
    "### load datasets\n",
    "    graphs=[]\n",
    "    # synthetic graphs\n",
    "    if args=='ladder':\n",
    "        graphs = []\n",
    "        for i in range(100, 201):\n",
    "            graphs.append(nx.ladder_graph(i))\n",
    "        max_prev_node = 10\n",
    "    elif args=='ladder_small':\n",
    "        graphs = []\n",
    "        for i in range(2, 11):\n",
    "            graphs.append(nx.ladder_graph(i))\n",
    "        max_prev_node = 10\n",
    "    elif args=='tree':\n",
    "        graphs = []\n",
    "        for i in range(2,5):\n",
    "            for j in range(3,5):\n",
    "                graphs.append(nx.balanced_tree(i,j))\n",
    "        max_prev_node = 256\n",
    "    elif args=='caveman':\n",
    "        # graphs = []\n",
    "        # for i in range(5,10):\n",
    "        #     for j in range(5,25):\n",
    "        #         for k in range(5):\n",
    "        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))\n",
    "        graphs = []\n",
    "        for i in range(2, 3):\n",
    "            for j in range(30, 81):\n",
    "                for k in range(10):\n",
    "                    graphs.append(caveman_special(i,j, p_edge=0.3))\n",
    "        max_prev_node = 100\n",
    "    elif args=='caveman_small':\n",
    "        # graphs = []\n",
    "        # for i in range(2,5):\n",
    "        #     for j in range(2,6):\n",
    "        #         for k in range(10):\n",
    "        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))\n",
    "        graphs = []\n",
    "        for i in range(2, 3):\n",
    "            for j in range(6, 11):\n",
    "                for k in range(20):\n",
    "                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8\n",
    "        max_prev_node = 20\n",
    "    elif args=='caveman_small_single':\n",
    "        # graphs = []\n",
    "        # for i in range(2,5):\n",
    "        #     for j in range(2,6):\n",
    "        #         for k in range(10):\n",
    "        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))\n",
    "        graphs = []\n",
    "        for i in range(2, 3):\n",
    "            for j in range(8, 9):\n",
    "                for k in range(100):\n",
    "                    graphs.append(caveman_special(i, j, p_edge=0.5))\n",
    "        max_prev_node = 20\n",
    "    elif args.startswith('community'):\n",
    "        num_communities = int(args[-1])\n",
    "        print('Creating dataset with ', num_communities, ' communities')\n",
    "        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)\n",
    "        #c_sizes = [15] * num_communities\n",
    "        for k in range(3000):\n",
    "            graphs.append(n_community(c_sizes, p_inter=0.01))\n",
    "        max_prev_node = 80\n",
    "    elif args=='grid':\n",
    "        graphs = []\n",
    "        for i in range(10,20):\n",
    "            for j in range(10,20):\n",
    "                graphs.append(nx.grid_2d_graph(i,j))\n",
    "        max_prev_node = 40\n",
    "    elif args=='grid_small':\n",
    "        graphs = []\n",
    "        for i in range(2,5):\n",
    "            for j in range(2,6):\n",
    "                graphs.append(nx.grid_2d_graph(i,j))\n",
    "        max_prev_node = 15\n",
    "    elif args=='barabasi':\n",
    "        graphs = []\n",
    "        for i in range(100,200):\n",
    "             for j in range(4,5):\n",
    "                 for k in range(5):\n",
    "                    graphs.append(nx.barabasi_albert_graph(i,j))\n",
    "        max_prev_node = 130\n",
    "    elif args=='barabasi_small':\n",
    "        graphs = []\n",
    "        for i in range(4,21):\n",
    "             for j in range(3,4):\n",
    "                 for k in range(10):\n",
    "                    graphs.append(nx.barabasi_albert_graph(i,j))\n",
    "        max_prev_node = 20\n",
    "    elif args=='grid_big':\n",
    "        graphs = []\n",
    "        for i in range(36, 46):\n",
    "            for j in range(36, 46):\n",
    "                graphs.append(nx.grid_2d_graph(i, j))\n",
    "        max_prev_node = 90\n",
    "\n",
    "    elif 'barabasi_noise' in args:\n",
    "        graphs = []\n",
    "        for i in range(100,101):\n",
    "            for j in range(4,5):\n",
    "                for k in range(500):\n",
    "                    graphs.append(nx.barabasi_albert_graph(i,j))\n",
    "        graphs = perturb_new(graphs,p=args.noise/10.0)\n",
    "        max_prev_node = 99\n",
    "\n",
    "    # real graphs\n",
    "    elif args == 'enzymes': ##TODO: Hier sind die Datensete\n",
    "        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')\n",
    "        max_prev_node = 25\n",
    "    elif args == 'enzymes_small':\n",
    "        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')\n",
    "        graphs = []\n",
    "        for G in graphs_raw:\n",
    "            if G.number_of_nodes()<=20:\n",
    "                graphs.append(G)\n",
    "        max_prev_node = 15\n",
    "    elif args == 'protein':\n",
    "        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')\n",
    "        max_prev_no.de = 80\n",
    "    elif args == 'DD':\n",
    "        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)\n",
    "        max_prev_node = 230\n",
    "    elif args == 'citeseer':\n",
    "        _, _, G = Graph_load(dataset='citeseer')\n",
    "        G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "        graphs = []\n",
    "        for i in range(G.number_of_nodes()):\n",
    "            G_ego = nx.ego_graph(G, i, radius=3)\n",
    "            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):\n",
    "                graphs.append(G_ego)\n",
    "        max_prev_node = 250\n",
    "    elif args == 'citeseer_small':\n",
    "        _, _, G = Graph_load(dataset='citeseer')\n",
    "        G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "        graphs = []\n",
    "        for i in range(G.number_of_nodes()):\n",
    "            G_ego = nx.ego_graph(G, i, radius=1)\n",
    "            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):\n",
    "                graphs.append(G_ego)\n",
    "        shuffle(graphs)\n",
    "        graphs = graphs[0:200]\n",
    "        max_prev_node = 15\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = create(\"ladder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben zurzeit keine Ordnung $ \\pi $, noch ist der Graph in Vektorform vorhanden. Über eine Breitesuche der Nachbarn bekommen eine\n",
    "nicht eindeutige Nummierung der Knoten. \n",
    "Die genaue Herangehensweiße ist:\n",
    "1. Wir wählen einen zufälligen Knoten des Graphen, dem die Knotenordnung die Zahl $1$ zuordnet.\n",
    "2. Wir wählen die Nachbarknoten dieses Knoten und $\\pi $ ordnet ihnen jeweils eine unterschiedliche Zahl zu.\n",
    "3. Wie in einer Breitensuche nehmen die Nachbarn dieser Knoten und ordnen diesen eine Zahl zu.\n",
    "\n",
    "Wir hören auf, wenn der Graph durchlaufen ist. Damit haben wir eine Knotenordnung $ \\pi $ gegeben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen die Kanten in eine spezielle Vektormenge überführen. Sei $ \\pi $ gegeben. Wir definieren $ S_i  \\in \\{ 0, 1 \\}^{i - 1} $, indem die j-te Eintrag $ 1 $ ist, falls $ \\pi(v_i) $ und $ \\pi(v_j) $ eine Kante besitzen, sonst $ 0 $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wandelt NX in SI - Arrays um\n",
    "def get_si(graph):\n",
    "    bfs_list = list()\n",
    "    q = Queue(maxsize = 0)\n",
    "    graph_list = list(graph.nodes())\n",
    "    start_element = random.choice(graph_list)\n",
    "    q.put(start_element)\n",
    "\n",
    "    while(q.qsize() > 0):\n",
    "        n = q.get()\n",
    "        if not n in bfs_list:\n",
    "            bfs_list.append(n)\n",
    "            neighs = list(nx.all_neighbors(graph, n))\n",
    "            for entry in neighs:\n",
    "                    q.put(entry)\n",
    "\n",
    "    result_si = list()\n",
    "    for i_list, i_node in enumerate(bfs_list[1:]):\n",
    "        neighs = list(nx.all_neighbors(graph, i_node))\n",
    "        si = list()\n",
    "\n",
    "        for i_vector_list, i_vector_node in enumerate(bfs_list[:i_list + 1]):\n",
    "            if i_vector_node in neighs:\n",
    "                si.append(1)\n",
    "            else:\n",
    "                si.append(0)\n",
    "        si_np = np.asarray(si)\n",
    "        result_si.append(si_np)\n",
    "    return result_si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](arch_graphrnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das neuronale Netzwerk besteht eigentlich aus den zwei Netzwerken $ f_{\\text{trans}} $ und $ f_{\\text{out}} $. $ f_{\\text{trans}} $ ist ein Gated Recurrend Unit, kurz GRU, welches als Input $ S_{i - 1} $ und den Hidden Vector $ h_{i - 1} $ hat und einen neuen Vektor $ h_i $ ausgibt, also $ h_i = f_{\\text{trans}} (S_{i - 1} , h_{i - 1}) $.\n",
    "\n",
    "$ f_{\\text{out}} $ nimmt dann den Hidden Vector $h_i $ und erstellt ein $ S_i $.\n",
    "$ f_{\\text{trans}} $ wird dabei als Kantennetzwerk betrachtet und $ f_{\\text{out}} $ als Knotennetzwerk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In (You et al. 2018) werden zwei Varianten vorgestellt: Einmal kann $ f_{\\text{out}} $ ein einfaches Multi-Layer Perceptron (wie hier implementiert) sein. Diese Variante wird in (You et al. 2018) GraphRNN - S genannt. Eine bessere Variante ist ein GRU für $ f_{\\text{out}} $ zu nehmen. Diese Variante wird dann GraphRNN in (You et al. 2018)  genannt. RNN meint hier ein Recurrend Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem unteren Codeblock erstellen wir das eigentliche Modell. Für das Knotenerstellung benutzen wir ein GRU, während die Kantenerstellung durch MLP entsteht. Der Hidden Vector hat die \n",
    "Größe 128 und max_nodes gibt die maximale Größe aller $ S_i $ an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOS (kurz für start of sequence) und EOS (kurz für end of sequence) sind vordefinierte Vektoren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOS zeigt den Netzwerk an, dass ein neuer Graph generiert werden soll und EOS zeigt an, wenn der Graph fertig generiert ist. Wir benutzen später das EOS, um anzuzeigen, dass der Graph fertig ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Ablauf im **Trainingsprozess** besteht wie folgt. Wir haben eine Liste an Graphen, wobei die Graphen \n",
    "wieder aus geordneten Listen von $ S_i $ - Vektoren bestehen. Wir nehmen einen Graphen und iterieren über ein $i$-tes $ S_i $, \n",
    "wobei wir mit $ S_0 $ anfangen.\n",
    "Am Anfang eines jeden Graphen geben wir SOS als Input in  $ f_{\\text{trans}}$. Den entstandenen Hidden Vector nehmen wir als Input\n",
    "für $ f_{\\text{out}} $ und erhalten einen Vektor. Die Verlustfunktion ist binary_cross_entropy und nimmt als Argument diesen Vektor und $ S_0 $.\n",
    "Danach wenden wir den Gradient Descent an.\n",
    "Im nächsten Schritt nehmen wir als Eingabe $ S_0 $ und den vorherigen Hidden Vector für $ f_{\\text{trans}}$.\n",
    "Die Ausgabe nehmen wir für $ f_{\\text{out}} $ und berechnen wieder den Loss mit $ S_1 $.\n",
    "\n",
    "Solange wir nicht den EOS erreichen, nehmen wir  $ S_i $ und den vorherigen Hidden Vector und berechnen den Loss mit der Ausgabe und  $ S_{i + 1} $.\n",
    "Falls der $ S_{i + 1} $ der EOS ist, beenden wir die Schleife und starten mit einem neuen Graphen von vorne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Ablauf im **Generierungsprozess** besteht wie folgt.\n",
    "Wir geben SOS als Input in  $ f_{\\text{trans}}$. Den entstandenen Hidden Vector nehmen wir als Input\n",
    "für $ f_{\\text{out}} $ und erhalten einen Vektor. Diesen Vektor nehmen wir als Parameter für eine binomiale Zufallsfunktion und erhalten einen Vektor, der $ S_0 $ sein soll.\n",
    "Im nächsten Schritt nehmen wir als Eingabe $ S_0 $ und den vorherigen Hidden Vector für $ f_{\\text{trans}}$.\n",
    "Die Ausgabe nehmen wir für $ f_{\\text{out}} $ und erhalten wieder ein Vektor. Mit diesen Vektor wenden wir eine binomiale Zufallsfunktion an\n",
    "und erhalten $ S_1 $.\n",
    "\n",
    "Solange wir nicht den EOS zurück bekommen, nehmen wir  $ S_i $ und den vorherigen Hidden Vector und berechnen  $ S_{i + 1} $.\n",
    "Falls der $ S_{i + 1} $ der EOS ist, beenden wir die Schleife und haben einen fertigen Graphen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modell\n",
    "class graphrnn_simple(nn.Module):\n",
    "    def __init__(self, max_nodes):\n",
    "        super(graphrnn_simple, self).__init__()\n",
    "        self.node_network = nn.GRUCell(128 + max_nodes, 128)\n",
    "        \n",
    "        \n",
    "        self.edge_mlp = nn.Linear(128, 500)\n",
    "        self.intern_act = nn.ReLU()\n",
    "        self.edge_mlp2 = nn.Linear(500, max_nodes)\n",
    "        self.act = nn.Sigmoid() \n",
    "        torch.nn.init.xavier_uniform_(self.node_network.weight_ih)\n",
    "        torch.nn.init.xavier_uniform_(self.node_network.weight_hh)\n",
    "        torch.nn.init.xavier_uniform_(self.edge_mlp.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.edge_mlp2.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.node_network(input)\n",
    "        self.hidden_vec = x.clone().detach()\n",
    "        x = self.edge_mlp(x)\n",
    "        x = self.intern_act(x)\n",
    "        x = self.edge_mlp2(x)\n",
    "        \n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOS und EOS\n",
    "def get_startvector(len_end): #hidden vec doch nur Nullen\n",
    "    start = torch.cat(\n",
    "        (torch.rand(128), torch.ones(len_end - 128))).view(1,len_end)\n",
    "    return start\n",
    "\n",
    "\n",
    "def get_endvector(len_end):\n",
    "    start = torch.zeros(len_end).view(1,len_end)\n",
    "    return start.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterator. Y ist X der nächsten Iteration\n",
    "def get_input_output_set(graph_set):\n",
    "    for j in range(1, len(graph_set)):\n",
    "        yield graph_set[j - 1], graph_set[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandelt numpy Si in Tensor um\n",
    "def si_to_tensor(si, length=124):\n",
    "    si_len = si.shape[0] #funtioniert so?\n",
    "    if si_len == length:\n",
    "        return torch.from_numpy(si).view(1, length).float()\n",
    "    si_torch = torch.from_numpy(si)\n",
    "    si_torch = torch.cat(\n",
    "        (si_torch, torch.zeros(length - si_len))\n",
    "    ).view(1, length)\n",
    "    return si_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set (graphs, n_clones):\n",
    "    return [get_si(random.choice(graphs)) for _ in range(n_clones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fügt EOS und SOS hinzu\n",
    "def prepare_set(graph_set, limit): \n",
    "    return [[np.ones(limit)] + g + [np.zeros(limit)]\n",
    "                for g in graph_set if len(g) <= limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## generiert Graphen, basierend auf dem Model\n",
    "def create_graphs(model, n_graphs, length=100):\n",
    "    created_graphs = list()\n",
    "    no_eos_graphs = list()\n",
    "    for k in range(100):\n",
    "        start = get_startvector(length + 128)\n",
    "        distribution = model(start)\n",
    "        \n",
    "        inputV = torch.cat(\n",
    "            (torch.FloatTensor(model.hidden_vec).view(1, 128), distribution.detach()), dim=1\n",
    "        )\n",
    "        i = 0\n",
    "        hs = list()\n",
    "        dss = list()\n",
    "        while(True):\n",
    "            hs.append(inputV)\n",
    "            distribution = model(inputV)\n",
    "            dss.append(torch.bernoulli(distribution))\n",
    "            inputV = torch.cat(\n",
    "                (torch.FloatTensor(model.hidden_vec).view(1, 128), distribution.detach()), dim=1\n",
    "            )\n",
    "            end = torch.from_numpy(get_endvector(distribution.shape[1]))\n",
    "            i += 1\n",
    "\n",
    "            ## Vermeidung von Endlosgenerierung\n",
    "            if i > n_graphs:\n",
    "                no_eos_graphs.append(dss)\n",
    "                break\n",
    "            if ( torch.round(distribution) == end).all():\n",
    "                hs.append(inputV)\n",
    "                if i > 1:\n",
    "                    created_graphs.append(dss)\n",
    "                    break\n",
    "                    \n",
    "    return created_graphs, no_eos_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wandelt BFS in NX Graphen um\n",
    "def si_to_nx(graph):\n",
    "    g = nx.Graph()\n",
    "    for i, si in enumerate(graph):\n",
    "        g.add_node(i)\n",
    "        if i == 0:\n",
    "            continue\n",
    "        si_local = si[0, :i].detach().numpy()\n",
    "        \n",
    "        ind = np.where(si_local == 1.)[0].tolist()\n",
    "        \n",
    "        for connec_ind in ind:\n",
    "            g.add_edge(connec_ind, i)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_to_nx_1d(graph):\n",
    "    g = nx.Graph()\n",
    "    for i, si in enumerate(graph):\n",
    "        g.add_node(i)\n",
    "        if i == 0:\n",
    "            continue\n",
    " \n",
    "        si_local = si[:i]\n",
    "        \n",
    "        ind = np.where(si_local == 1.)[0].tolist()\n",
    "        \n",
    "        for connec_ind in ind:\n",
    "            g.add_edge(connec_ind, i)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Konstanten\n",
    "training_split = int(len(graphs) * 0.7)\n",
    "val_split = int(len(graphs) * 0.85)\n",
    "\n",
    "N_BFS_CLONES = 1000\n",
    "N_VAL_CLONES = 100\n",
    "\n",
    "QUANTILE_LIMIT = int((N_BFS_CLONES + 2 * N_VAL_CLONES) * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = get_set(graphs[:training_split], N_BFS_CLONES)\n",
    "validation_set = get_set(graphs[training_split:val_split], N_VAL_CLONES)\n",
    "test_set = get_set(graphs[val_split:], N_VAL_CLONES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## erstellt quantile, um zu große Vektoren auszufiltern\n",
    "len_set = [len(g) for g in training_set + validation_set + test_set]\n",
    "# Das MLP braucht eine feste größe Eingabegröße. Wir nehmen den 95% - Quantil und verwerfen den Rest der Eingaben\n",
    "quantile = sorted(len_set)[QUANTILE_LIMIT]\n",
    "training_set = prepare_set(training_set, quantile)\n",
    "validation_set = prepare_set(validation_set, quantile)\n",
    "test_set = prepare_set(test_set, quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir übernehmen die Validierungsmetriken von You et. al, 2018 und verweisen für weitere Details dorthin. Die Implementierung der Metriken stammt von https://github.com/snap-stanford/GraphRNN/blob/master/eval/stats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run eval/stats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = graphrnn_simple(quantile)\n",
    "\n",
    "criterion = F.binary_cross_entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Trainingsprozess läuft so ab, dass wir $ S_{ i - 1} $ und den Hiddenvector nehmen und als Ausgabe $ S_{ i} $ erwarten.\n",
    "Am Anfang nehmen wir SOS und einen zufälligen Vektor statt  $ S_{ i - 1} $ und den Hiddenvector, und am Ende erwarten wir ein EOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "last_validation_step = 0\n",
    "VALIDATE_AFTER = 2000\n",
    "val_dg = 0\n",
    "val_cs = 0\n",
    "interesting_models = list()\n",
    "cut_training_set = 100\n",
    "for epoch in range(2):\n",
    "    min_loss = 2\n",
    "\n",
    "    for i, g in enumerate(training_set[:cut_training_set]):\n",
    "\n",
    "    \n",
    "        running_loss = 0\n",
    "\n",
    "        for i_one_graph, (si_input, si_output) in enumerate(get_input_output_set(g)):\n",
    "            tensor_input = si_to_tensor(si_input, length=quantile)\n",
    "            if i_one_graph == 0:\n",
    "                inputV = torch.cat(\n",
    "                (torch.rand(1, 128), tensor_input.detach()), dim=1\n",
    "                ) \n",
    "            else:\n",
    "                inputV = torch.cat(\n",
    "                (torch.FloatTensor(model.hidden_vec).view(1, 128), tensor_input.detach()), dim=1\n",
    "                ) \n",
    "\n",
    "            distribution = model(inputV)\n",
    "            \n",
    "            tensor_output = si_to_tensor(si_output, length=quantile)\n",
    "            loss = criterion(distribution, tensor_output)\n",
    "\n",
    "            loss.backward()\n",
    "            inputV = torch.cat(\n",
    "                (torch.FloatTensor(model.hidden_vec).view(1, 128), tensor_output.detach()), dim=1\n",
    "            ) \n",
    "\n",
    "            writer.add_scalar(\"LOSS\", loss.item(), step )\n",
    "            step += 1\n",
    "            print(f\"graph {i} \\t EPOCH: {epoch} \\t loss  {loss.item():.5f} \\t step: {step}\", end=\"\\r\")\n",
    "\n",
    "            if loss.item() < min_loss:\n",
    "                min_loss = loss.item()\n",
    "                best_model = copy(model)\n",
    "        distribution = model(inputV)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## validation erstellt Graphen, um sie dann zu validieren\n",
    "        if step - last_validation_step >= VALIDATE_AFTER:\n",
    "            last_validation_step = step\n",
    "            syn_graph, _ = create_graphs(model, 10, length=quantile)\n",
    "            if len(syn_graph) == 0 or step == 0:\n",
    "                print(\"\\n synthectic 0 \\n\")\n",
    "                writer.add_scalar(\"Degree Staats\", 2, step )\n",
    "                writer.add_scalar(\"Clustering Stats\", 0, step )\n",
    "                continue\n",
    "            nx_generated_graphs = [si_to_nx(g) for g in syn_graph]\n",
    "\n",
    "            val_dg = degree_stats(nx_generated_graphs, graphs[training_split:val_split])\n",
    "            val_cs = clustering_stats(nx_generated_graphs, graphs[training_split:val_split])\n",
    "            \n",
    "            if val_cs > 0 or val_dg < 2 or len(syn_graph) > 0:\n",
    "                interesting_models.append(copy(model))\n",
    "            \n",
    "            writer.add_scalar(\"Degree Staats\", val_dg, step )\n",
    "            writer.add_scalar(\"Clustering Stats\", val_cs, step )\n",
    "            \n",
    "\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs, _ = create_graphs(model, 300, length=quantile)\n",
    "nx_generated_graphs = [si_to_nx(g) for g in test_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_set = graphs[val_split:]\n",
    "val_dg = degree_stats(nx_generated_graphs, test_set)\n",
    "val_cs = clustering_stats(nx_generated_graphs, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"bestmodels/best_model_tree.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"bestmodels/best_model_tree.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quelle: https://github.com/snap-stanford/GraphRNN/blob/master/utils.py\n",
    "def draw_graph_list(G_list, row, col, fname = 'figures/test', layout='spring', is_single=False,k=1,node_size=55,alpha=1,width=1.3):\n",
    "    # # draw graph view\n",
    "    # from pylab import rcParams\n",
    "    # rcParams['figure.figsize'] = 12,3\n",
    "    plt.switch_backend('agg')\n",
    "    for i,G in enumerate(G_list[:row * col]):\n",
    "        plt.subplot(row,col,i+1)\n",
    "        plt.subplots_adjust(left=0, bottom=0, right=1, top=1,\n",
    "                        wspace=0, hspace=0)\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        if layout=='spring':\n",
    "            pos = nx.spring_layout(G,k=k/np.sqrt(G.number_of_nodes()),iterations=100)\n",
    "            # pos = nx.spring_layout(G)\n",
    "\n",
    "        elif layout=='spectral':\n",
    "            pos = nx.spectral_layout(G)\n",
    "        # # nx.draw_networkx(G, with_labels=True, node_size=2, width=0.15, font_size = 1.5, node_color=colors,pos=pos)\n",
    "        # nx.draw_networkx(G, with_labels=False, node_size=1.5, width=0.2, font_size = 1.5, linewidths=0.2, node_color = 'k',pos=pos,alpha=0.2)\n",
    "\n",
    "        if is_single:\n",
    "            # node_size default 60, edge_width default 1.5\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='#336699', alpha=1, linewidths=0, font_size=0)\n",
    "            nx.draw_networkx_edges(G, pos, alpha=alpha, width=width)\n",
    "        else:\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=1.5, node_color='#336699',alpha=1, linewidths=0.2)\n",
    "            nx.draw_networkx_edges(G, pos, alpha=0.3,width=0.2)\n",
    "\n",
    "        # plt.axis('off')\n",
    "        # plt.title('Complete Graph of Odd-degree Nodes')\n",
    "        # plt.show()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(fname+'.png', dpi=600)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-5af41a2067aa>:34: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "draw_graph_list(graphs[val_split:], 1, 1, fname=\"figures/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werde ich die Ergebnisse der Netzwerke vorstellen. Erstmal werden die Testdaten visualisiert. Dann folgen Diagramme,\n",
    "die den Verlauf der Loss - Kurve und der Validierungsmetriken Degree Stat und Cluster Stat beschreiben.\n",
    "Zuletzt visualisieren wir die generierten Graphen.\n",
    "\"ladder\" und \"tree\" sind Argumente der Funktion create. Auf diesen Graphen stützen sich die Untersuchungen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ladder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben hier 100 Graphen pro Epoche genommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisierung der Testdaten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ladder_test.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_loss0720.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_loss_log.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Cluster Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_clustering_stats0720.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Degree Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_degree_stats.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisierung der generierten Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ladder_synth.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisierung der Testdaten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tree_test.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/loss_tree.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/loss_log_tree.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Cluster Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/cs_tree.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurve Degree Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ds_tree.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisierung der generierten Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tree_synth.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse des Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Paper wurden andere Graphen generiert als in diesem Notebook. Deswegen vergleichen wir nicht mit den Paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diskussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Generierung ist definitiv ausbaufähig, wie anhand der Visualisierung feststellen können. Bei den Baumgraphen werden unzureichende Graphen generiert,während für\n",
    "Leitergraphen wir schon gute Ergebnisse erzielen.\n",
    "Eine Möglichkeit wäre es, statt GraphRNN simple die komplexere Variante zu nehmen. Weiterhin wäre es nützlich, länger mit besserer Hardware zu trainieren. Die Trainingshardware war ein Carbon X1 aus dem Jahr 2018 mit Intel® Core™ i7-7500U CPU @ 2.70GHz. Wir haben GraphRNN - simple, da alleine die Implementierung schon sehr zeitaufwendig war und einfacheres Modell mit schwächerer Hardware trainierbarer ist.\n",
    "für\n",
    "Weiterhin hatte ich mit einer Reihe weiterer Probleme zu kämpfen. Ich konnte nicht mit beliebig vielen Daten trainieren, denn wenn ein Modell zu overfittet war, neigte es\n",
    "dazu, kein EOS mehr zu senden. Ausserdem empfand ich es als Widerspruch, dass im Paper nicht zwischen Generierung und Training unterschieden worden ist.\n",
    "Im Paper wurde das Modell mit einem Zufallsprozess erläutert, aber verschwiegen, dass für den Trainingsprozess dieser wegfällt. Erst ein Blick in die konkrete Implementierung verstand ich diesen Unterschied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J. You et al, 2018. GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models In: ICML 2018\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "12780242bca3633304f3c6ef49572db24fb29261a62a3a29d2d842ead370ab42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
