{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of the paper \"GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models\".\n",
    "The aim is to implement a neural network that is fed with graphs beforehand and then generates new ones.\n",
    "An example application would be to take organic molecules as graphs and then create new potential compounds (You et al. 2018).\n",
    "(You et al. 2018) .\n",
    ".\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An undirected graph $G = (V, E)$ is defined by its vertex set\n",
    "$V = \\{v_1 , \\dots , v_n \\} $ and the edge set $E \\subset \\{(v_i , v_j ) \\mid v_i , v_j \\in V \\} $.\n",
    "Furthermore, we want to define a node order $ \\pi \\colon V \\to \\mathbb{N} $ that assigns a natural number to each node and is an injective function.\n",
    "Using the node order, we can represent each graph $G$ by an adjacency matrix $ A^\\pi \\in \\mathbb{R}^{n \\times n} $ where the $ (k, l) $ entry is equal to $ 1 $ if if $ \\pi(v_i) = k $ and $ \\pi(v_j) = l $, then $ (v_i, v_j) \\in E $ holds, and otherwise the entry is $ 0 $. Let $ \\Pi $ be the set of all node orders of the graph $ G $.\n",
    "\n",
    "The goal of learning generative models of graphs is to learn\n",
    "a distribution $p_{\\text{model}} (\\tilde{G}) $ over graphs based on a set of\n",
    "observed graphs $\\tilde{G} = \\{G_1 , \\dots, G_s \\} $ coming from the known distribution $p$, where each graph $ G_i $ has a different finite number of nodes and edges.\n",
    "number of nodes and edges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDas Ausf√ºhren von Zellen mit \"/bin/python3\" erfordert das Paket ipykernel.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout, to_agraph\n",
    "from queue import Queue\n",
    "from random import choice, randint\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from eval import stats\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from copy import copy\n",
    "#from data import *\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates data for validation, training and testing\n",
    "## Quelle: https://github.com/snap-stanford/GraphRNN/blob/master/create_graphs.py\n",
    "def create(args):\n",
    "### load datasets\n",
    "    graphs=[]\n",
    "    # synthetic graphs\n",
    "    if args=='ladder':\n",
    "        graphs = []\n",
    "        for i in range(100, 201):\n",
    "            graphs.append(nx.ladder_graph(i))\n",
    "        max_prev_node = 10\n",
    "    elif args=='ladder_small':\n",
    "        graphs = []\n",
    "        for i in range(2, 11):\n",
    "            graphs.append(nx.ladder_graph(i))\n",
    "        max_prev_node = 10\n",
    "    elif args=='tree':\n",
    "        graphs = []\n",
    "        for i in range(2,5):\n",
    "            for j in range(3,5):\n",
    "                graphs.append(nx.balanced_tree(i,j))\n",
    "        max_prev_node = 256\n",
    "    elif args=='caveman':\n",
    "        # graphs = []\n",
    "        # for i in range(5,10):\n",
    "        #     for j in range(5,25):\n",
    "        #         for k in range(5):\n",
    "        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))\n",
    "        graphs = []\n",
    "        for i in range(2, 3):\n",
    "            for j in range(30, 81):\n",
    "                for k in range(10):\n",
    "                    graphs.append(caveman_special(i,j, p_edge=0.3))\n",
    "        max_prev_node = 100\n",
    "    elif args=='caveman_small':\n",
    "        # graphs = []\n",
    "        # for i in range(2,5):\n",
    "        #     for j in range(2,6):\n",
    "        #         for k in range(10):\n",
    "        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))\n",
    "        graphs = []\n",
    "        for i in range(2, 3):\n",
    "            for j in range(6, 11):\n",
    "                for k in range(20):\n",
    "                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8\n",
    "        max_prev_node = 20\n",
    "    elif args=='caveman_small_single':\n",
    "        # graphs = []\n",
    "        # for i in range(2,5):\n",
    "        #     for j in range(2,6):\n",
    "        #         for k in range(10):\n",
    "        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))\n",
    "        graphs = []\n",
    "        for i in range(2, 3):\n",
    "            for j in range(8, 9):\n",
    "                for k in range(100):\n",
    "                    graphs.append(caveman_special(i, j, p_edge=0.5))\n",
    "        max_prev_node = 20\n",
    "    elif args.startswith('community'):\n",
    "        num_communities = int(args[-1])\n",
    "        print('Creating dataset with ', num_communities, ' communities')\n",
    "        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)\n",
    "        #c_sizes = [15] * num_communities\n",
    "        for k in range(3000):\n",
    "            graphs.append(n_community(c_sizes, p_inter=0.01))\n",
    "        max_prev_node = 80\n",
    "    elif args=='grid':\n",
    "        graphs = []\n",
    "        for i in range(10,20):\n",
    "            for j in range(10,20):\n",
    "                graphs.append(nx.grid_2d_graph(i,j))\n",
    "        max_prev_node = 40\n",
    "    elif args=='grid_small':\n",
    "        graphs = []\n",
    "        for i in range(2,5):\n",
    "            for j in range(2,6):\n",
    "                graphs.append(nx.grid_2d_graph(i,j))\n",
    "        max_prev_node = 15\n",
    "    elif args=='barabasi':\n",
    "        graphs = []\n",
    "        for i in range(100,200):\n",
    "             for j in range(4,5):\n",
    "                 for k in range(5):\n",
    "                    graphs.append(nx.barabasi_albert_graph(i,j))\n",
    "        max_prev_node = 130\n",
    "    elif args=='barabasi_small':\n",
    "        graphs = []\n",
    "        for i in range(4,21):\n",
    "             for j in range(3,4):\n",
    "                 for k in range(10):\n",
    "                    graphs.append(nx.barabasi_albert_graph(i,j))\n",
    "        max_prev_node = 20\n",
    "    elif args=='grid_big':\n",
    "        graphs = []\n",
    "        for i in range(36, 46):\n",
    "            for j in range(36, 46):\n",
    "                graphs.append(nx.grid_2d_graph(i, j))\n",
    "        max_prev_node = 90\n",
    "\n",
    "    elif 'barabasi_noise' in args:\n",
    "        graphs = []\n",
    "        for i in range(100,101):\n",
    "            for j in range(4,5):\n",
    "                for k in range(500):\n",
    "                    graphs.append(nx.barabasi_albert_graph(i,j))\n",
    "        graphs = perturb_new(graphs,p=args.noise/10.0)\n",
    "        max_prev_node = 99\n",
    "\n",
    "    # real graphs\n",
    "    elif args == 'enzymes': \n",
    "        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')\n",
    "        max_prev_node = 25\n",
    "    elif args == 'enzymes_small':\n",
    "        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')\n",
    "        graphs = []\n",
    "        for G in graphs_raw:\n",
    "            if G.number_of_nodes()<=20:\n",
    "                graphs.append(G)\n",
    "        max_prev_node = 15\n",
    "    elif args == 'protein':\n",
    "        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')\n",
    "        max_prev_no.de = 80\n",
    "    elif args == 'DD':\n",
    "        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)\n",
    "        max_prev_node = 230\n",
    "    elif args == 'citeseer':\n",
    "        _, _, G = Graph_load(dataset='citeseer')\n",
    "        G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "        graphs = []\n",
    "        for i in range(G.number_of_nodes()):\n",
    "            G_ego = nx.ego_graph(G, i, radius=3)\n",
    "            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):\n",
    "                graphs.append(G_ego)\n",
    "        max_prev_node = 250\n",
    "    elif args == 'citeseer_small':\n",
    "        _, _, G = Graph_load(dataset='citeseer')\n",
    "        G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "        graphs = []\n",
    "        for i in range(G.number_of_nodes()):\n",
    "            G_ego = nx.ego_graph(G, i, radius=1)\n",
    "            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):\n",
    "                graphs.append(G_ego)\n",
    "        shuffle(graphs)\n",
    "        graphs = graphs[0:200]\n",
    "        max_prev_node = 15\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = create(\"ladder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have no order $ \\pi $, nor is the graph in vector form. Using a breadth-first search of the neighbours we get a\n",
    "indeterminate numbering of the nodes. \n",
    "The exact approach is:\n",
    "1. we choose a random node of the graph to which the node order assigns the number $1$.\n",
    "2. we choose the neighbouring nodes of this node and $\\pi $ assigns a different number to each of them.\n",
    "3. as in a breadth-first search, take the neighbours of these nodes and assign a number to them.\n",
    "\n",
    "We stop when the graph is traversed. Thus we have given a node order $ \\pi $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transform the edges into a special vector set. Let $ \\pi $ be given. We define $ S_i \\in \\{ 0, 1 \\}^{i - 1} $ in that the jth entry is $ 1 $ if $ \\pi(v_i) $ and $ \\pi(v_j) $ have an edge, otherwise $ 0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changes NX into SI - Arrays\n",
    "def get_si(graph):\n",
    "    bfs_list = list()\n",
    "    q = Queue(maxsize = 0)\n",
    "    graph_list = list(graph.nodes())\n",
    "    start_element = random.choice(graph_list)\n",
    "    q.put(start_element)\n",
    "\n",
    "    while(q.qsize() > 0):\n",
    "        n = q.get()\n",
    "        if not n in bfs_list:\n",
    "            bfs_list.append(n)\n",
    "            neighs = list(nx.all_neighbors(graph, n))\n",
    "            for entry in neighs:\n",
    "                    q.put(entry)\n",
    "\n",
    "    result_si = list()\n",
    "    for i_list, i_node in enumerate(bfs_list[1:]):\n",
    "        neighs = list(nx.all_neighbors(graph, i_node))\n",
    "        si = list()\n",
    "\n",
    "        for i_vector_list, i_vector_node in enumerate(bfs_list[:i_list + 1]):\n",
    "            if i_vector_node in neighs:\n",
    "                si.append(1)\n",
    "            else:\n",
    "                si.append(0)\n",
    "        si_np = np.asarray(si)\n",
    "        result_si.append(si_np)\n",
    "    return result_si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](arch_graphrnn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network actually consists of the two networks $ f_{\\text{trans}} $ and $ f_{\\text{out}} $. $ f_{\\text{trans}} $ is a gated recurrent unit, GRU for short, which has as input $ S_{i - 1} $ and the hidden vector $ h_{i - 1} $ and outputs a new vector $ h_i $, i.e. $ h_i = f_{\\text{trans}} (S_{i - 1} , h_{i - 1}) $.\n",
    "\n",
    "$ f_{\\text{out}} $ then takes the hidden vector $h_i $ and creates an $ S_i $.\n",
    "$ f_{\\text{trans}} $ is considered as an edge network and $ f_{\\text{out}} $ as a node network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In (You et al. 2018), two variants are presented: Once, $ f_{\\text{out}} $ can be a simple multi-layer perceptron (as implemented here). This variant is called GraphRNN - S in (You et al. 2018). A better variant is to take a GRU for $ f_{\\text{out}} $. This variant is then called GraphRNN in (You et al. 2018). RNN here means a Recurrent Neural Network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below we create the actual model. For the node creation we use a GRU, while the edge creation is created by MLP. The hidden vector has the \n",
    "size 128 and max_nodes specifies the maximum size of all $ S_i $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOS (short for start of sequence) and EOS (short for end of sequence) are predefined vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOS indicates to the network that a new graph is to be generated and EOS indicates when the graph is finished being generated. We use the EOS later to indicate that the graph is ready."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow in the **training process** is as follows. We have a list of graphs, where the graphs are \n",
    "again consist of ordered lists of $ S_i $ vectors. We take a graph and iterate over an $i$-th $ S_i $, \n",
    "starting with $ S_0 $.\n",
    "At the beginning of each graph, we give SOS as input into $ f_{\\text{trans}}$. We take the resulting hidden vector as input\n",
    "for $ f_{\\text{out}} $ and get a vector. The loss function is binary_cross_entropy and takes as argument this vector and $ S_0 $.\n",
    "Then we apply the gradient descent.\n",
    "In the next step, we take as input $ S_0 $ and the previous hidden vector for $ f_{\\text{trans}}$.\n",
    "We take the output for $ f_{\\text{out}} $ and calculate the loss again with $ S_1 $.\n",
    "\n",
    "As long as we do not reach the EOS, we take $ S_i $ and the previous hidden vector and calculate the loss with the output and $ S_{i + 1} $.\n",
    "If $ S_{i + 1} $ is the EOS, we end the loop and start again with a new graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow in the **generation process** is as follows.\n",
    "We give SOS as input in $ f_{\\text{trans}}$. We take the resulting hidden vector as input\n",
    "for $ f_{\\text{out}} $ and get a vector. We take this vector as a parameter for a binomial random function and get a vector that should be $ S_0 $.\n",
    "In the next step, we take as input $ S_0 $ and the previous hidden vector for $ f_{\\text{trans}}$.\n",
    "We take the output for $ f_{\\text{out}} $ and get a vector again. With this vector we apply a binomial random function\n",
    "and get $ S_1 $.\n",
    "\n",
    "As long as we do not get the EOS back, we take $ S_i $ and the previous hidden vector and calculate $ S_{i + 1} $.\n",
    "If the $ S_{i + 1} $ is the EOS, we end the loop and have a finished graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "class graphrnn_simple(nn.Module):\n",
    "    def __init__(self, max_nodes):\n",
    "        super(graphrnn_simple, self).__init__()\n",
    "        self.node_network = nn.GRUCell(128 + max_nodes, 128)\n",
    "        \n",
    "        \n",
    "        self.edge_mlp = nn.Linear(128, 500)\n",
    "        self.intern_act = nn.ReLU()\n",
    "        self.edge_mlp2 = nn.Linear(500, max_nodes)\n",
    "        self.act = nn.Sigmoid() \n",
    "        torch.nn.init.xavier_uniform_(self.node_network.weight_ih)\n",
    "        torch.nn.init.xavier_uniform_(self.node_network.weight_hh)\n",
    "        torch.nn.init.xavier_uniform_(self.edge_mlp.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.edge_mlp2.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.node_network(input)\n",
    "        self.hidden_vec = x.clone().detach()\n",
    "        x = self.edge_mlp(x)\n",
    "        x = self.intern_act(x)\n",
    "        x = self.edge_mlp2(x)\n",
    "        \n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOS and EOS\n",
    "def get_startvector(len_end): #hidden vec only zeros\n",
    "    start = torch.cat(\n",
    "        (torch.rand(128), torch.ones(len_end - 128))).view(1,len_end)\n",
    "    return start\n",
    "\n",
    "\n",
    "def get_endvector(len_end):\n",
    "    start = torch.zeros(len_end).view(1,len_end)\n",
    "    return start.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterator. Y is X of next iteration\n",
    "def get_input_output_set(graph_set):\n",
    "    for j in range(1, len(graph_set)):\n",
    "        yield graph_set[j - 1], graph_set[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes np into tensor\n",
    "def si_to_tensor(si, length=124):\n",
    "    si_len = si.shape[0] \n",
    "    if si_len == length:\n",
    "        return torch.from_numpy(si).view(1, length).float()\n",
    "    si_torch = torch.from_numpy(si)\n",
    "    si_torch = torch.cat(\n",
    "        (si_torch, torch.zeros(length - si_len))\n",
    "    ).view(1, length)\n",
    "    return si_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set (graphs, n_clones):\n",
    "    return [get_si(random.choice(graphs)) for _ in range(n_clones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adds EOS and SOS\n",
    "def prepare_set(graph_set, limit): \n",
    "    return [[np.ones(limit)] + g + [np.zeros(limit)]\n",
    "                for g in graph_set if len(g) <= limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## generates graph based on model\n",
    "def create_graphs(model, n_graphs, length=100):\n",
    "    created_graphs = list()\n",
    "    no_eos_graphs = list()\n",
    "    for k in range(100):\n",
    "        start = get_startvector(length + 128)\n",
    "        distribution = model(start)\n",
    "        \n",
    "        inputV = torch.cat(\n",
    "            (torch.FloatTensor(model.hidden_vec).view(1, 128), distribution.detach()), dim=1\n",
    "        )\n",
    "        i = 0\n",
    "        hs = list()\n",
    "        dss = list()\n",
    "        while(True):\n",
    "            hs.append(inputV)\n",
    "            distribution = model(inputV)\n",
    "            dss.append(torch.bernoulli(distribution))\n",
    "            inputV = torch.cat(\n",
    "                (torch.FloatTensor(model.hidden_vec).view(1, 128), distribution.detach()), dim=1\n",
    "            )\n",
    "            end = torch.from_numpy(get_endvector(distribution.shape[1]))\n",
    "            i += 1\n",
    "\n",
    "            ## Vermeidung von Endlosgenerierung\n",
    "            if i > n_graphs:\n",
    "                no_eos_graphs.append(dss)\n",
    "                break\n",
    "            if ( torch.round(distribution) == end).all():\n",
    "                hs.append(inputV)\n",
    "                if i > 1:\n",
    "                    created_graphs.append(dss)\n",
    "                    break\n",
    "                    \n",
    "    return created_graphs, no_eos_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##changes bfs graphs into nx\n",
    "def si_to_nx(graph):\n",
    "    g = nx.Graph()\n",
    "    for i, si in enumerate(graph):\n",
    "        g.add_node(i)\n",
    "        if i == 0:\n",
    "            continue\n",
    "        si_local = si[0, :i].detach().numpy()\n",
    "        \n",
    "        ind = np.where(si_local == 1.)[0].tolist()\n",
    "        \n",
    "        for connec_ind in ind:\n",
    "            g.add_edge(connec_ind, i)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_to_nx_1d(graph):\n",
    "    g = nx.Graph()\n",
    "    for i, si in enumerate(graph):\n",
    "        g.add_node(i)\n",
    "        if i == 0:\n",
    "            continue\n",
    " \n",
    "        si_local = si[:i]\n",
    "        \n",
    "        ind = np.where(si_local == 1.)[0].tolist()\n",
    "        \n",
    "        for connec_ind in ind:\n",
    "            g.add_edge(connec_ind, i)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "training_split = int(len(graphs) * 0.7)\n",
    "val_split = int(len(graphs) * 0.85)\n",
    "\n",
    "N_BFS_CLONES = 1000\n",
    "N_VAL_CLONES = 100\n",
    "\n",
    "QUANTILE_LIMIT = int((N_BFS_CLONES + 2 * N_VAL_CLONES) * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = get_set(graphs[:training_split], N_BFS_CLONES)\n",
    "validation_set = get_set(graphs[training_split:val_split], N_VAL_CLONES)\n",
    "test_set = get_set(graphs[val_split:], N_VAL_CLONES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates quantiles to filter to big graphs\n",
    "len_set = [len(g) for g in training_set + validation_set + test_set]\n",
    "# mlp needs fixed sized input so choose a 95 % size.\n",
    "quantile = sorted(len_set)[QUANTILE_LIMIT]\n",
    "training_set = prepare_set(training_set, quantile)\n",
    "validation_set = prepare_set(validation_set, quantile)\n",
    "test_set = prepare_set(test_set, quantile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adopt the validation metrics from You et. al, 2018 and refer there for further details. The implementation of the metrics comes from https://github.com/snap-stanford/GraphRNN/blob/master/eval/stats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run eval/stats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = graphrnn_simple(quantile)\n",
    "\n",
    "criterion = F.binary_cross_entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is such that we take $ S_{ i - 1} $ and the hidden vector and expect $ S_{ i} $ as output.\n",
    "At the beginning we take SOS and a random vector instead of $ S_{ i - 1} $ and the hidden vector, and at the end we expect an EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "last_validation_step = 0\n",
    "VALIDATE_AFTER = 2000\n",
    "val_dg = 0\n",
    "val_cs = 0\n",
    "interesting_models = list()\n",
    "cut_training_set = 100\n",
    "for epoch in range(2):\n",
    "    min_loss = 2\n",
    "\n",
    "    for i, g in enumerate(training_set[:cut_training_set]):\n",
    "\n",
    "    \n",
    "        running_loss = 0\n",
    "\n",
    "        for i_one_graph, (si_input, si_output) in enumerate(get_input_output_set(g)):\n",
    "            tensor_input = si_to_tensor(si_input, length=quantile)\n",
    "            if i_one_graph == 0:\n",
    "                inputV = torch.cat(\n",
    "                (torch.rand(1, 128), tensor_input.detach()), dim=1\n",
    "                ) \n",
    "            else:\n",
    "                inputV = torch.cat(\n",
    "                (torch.FloatTensor(model.hidden_vec).view(1, 128), tensor_input.detach()), dim=1\n",
    "                ) \n",
    "\n",
    "            distribution = model(inputV)\n",
    "            \n",
    "            tensor_output = si_to_tensor(si_output, length=quantile)\n",
    "            loss = criterion(distribution, tensor_output)\n",
    "\n",
    "            loss.backward()\n",
    "            inputV = torch.cat(\n",
    "                (torch.FloatTensor(model.hidden_vec).view(1, 128), tensor_output.detach()), dim=1\n",
    "            ) \n",
    "\n",
    "            writer.add_scalar(\"LOSS\", loss.item(), step )\n",
    "            step += 1\n",
    "            print(f\"graph {i} \\t EPOCH: {epoch} \\t loss  {loss.item():.5f} \\t step: {step}\", end=\"\\r\")\n",
    "\n",
    "            if loss.item() < min_loss:\n",
    "                min_loss = loss.item()\n",
    "                best_model = copy(model)\n",
    "        distribution = model(inputV)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## validation creates graph for validation\n",
    "        if step - last_validation_step >= VALIDATE_AFTER:\n",
    "            last_validation_step = step\n",
    "            syn_graph, _ = create_graphs(model, 10, length=quantile)\n",
    "            if len(syn_graph) == 0 or step == 0:\n",
    "                print(\"\\n synthectic 0 \\n\")\n",
    "                writer.add_scalar(\"Degree Staats\", 2, step )\n",
    "                writer.add_scalar(\"Clustering Stats\", 0, step )\n",
    "                continue\n",
    "            nx_generated_graphs = [si_to_nx(g) for g in syn_graph]\n",
    "\n",
    "            val_dg = degree_stats(nx_generated_graphs, graphs[training_split:val_split])\n",
    "            val_cs = clustering_stats(nx_generated_graphs, graphs[training_split:val_split])\n",
    "            \n",
    "            if val_cs > 0 or val_dg < 2 or len(syn_graph) > 0:\n",
    "                interesting_models.append(copy(model))\n",
    "            \n",
    "            writer.add_scalar(\"Degree Staats\", val_dg, step )\n",
    "            writer.add_scalar(\"Clustering Stats\", val_cs, step )\n",
    "            \n",
    "\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs, _ = create_graphs(model, 300, length=quantile)\n",
    "nx_generated_graphs = [si_to_nx(g) for g in test_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_set = graphs[val_split:]\n",
    "val_dg = degree_stats(nx_generated_graphs, test_set)\n",
    "val_cs = clustering_stats(nx_generated_graphs, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"bestmodels/best_model_tree.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"bestmodels/best_model_tree.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## source: https://github.com/snap-stanford/GraphRNN/blob/master/utils.py\n",
    "def draw_graph_list(G_list, row, col, fname = 'figures/test', layout='spring', is_single=False,k=1,node_size=55,alpha=1,width=1.3):\n",
    "    # # draw graph view\n",
    "    # from pylab import rcParams\n",
    "    # rcParams['figure.figsize'] = 12,3\n",
    "    plt.switch_backend('agg')\n",
    "    for i,G in enumerate(G_list[:row * col]):\n",
    "        plt.subplot(row,col,i+1)\n",
    "        plt.subplots_adjust(left=0, bottom=0, right=1, top=1,\n",
    "                        wspace=0, hspace=0)\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        if layout=='spring':\n",
    "            pos = nx.spring_layout(G,k=k/np.sqrt(G.number_of_nodes()),iterations=100)\n",
    "            # pos = nx.spring_layout(G)\n",
    "\n",
    "        elif layout=='spectral':\n",
    "            pos = nx.spectral_layout(G)\n",
    "        # # nx.draw_networkx(G, with_labels=True, node_size=2, width=0.15, font_size = 1.5, node_color=colors,pos=pos)\n",
    "        # nx.draw_networkx(G, with_labels=False, node_size=1.5, width=0.2, font_size = 1.5, linewidths=0.2, node_color = 'k',pos=pos,alpha=0.2)\n",
    "\n",
    "        if is_single:\n",
    "            # node_size default 60, edge_width default 1.5\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='#336699', alpha=1, linewidths=0, font_size=0)\n",
    "            nx.draw_networkx_edges(G, pos, alpha=alpha, width=width)\n",
    "        else:\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=1.5, node_color='#336699',alpha=1, linewidths=0.2)\n",
    "            nx.draw_networkx_edges(G, pos, alpha=0.3,width=0.2)\n",
    "\n",
    "        # plt.axis('off')\n",
    "        # plt.title('Complete Graph of Odd-degree Nodes')\n",
    "        # plt.show()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(fname+'.png', dpi=600)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-5af41a2067aa>:34: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "draw_graph_list(graphs[val_split:], 1, 1, fname=\"figures/test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will present the results of the networks. First, the test data is visualised. This is followed by diagrams,\n",
    "describing the loss curve and the validation metrics Degree Stat and Cluster Stat.\n",
    "Finally, we visualise the generated graphs.\n",
    "\"Ladder\" and \"tree\" are arguments of the create function. The investigations are based on this graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ladder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken 100 graphs per epoch here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualisation test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ladder_test.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_loss0720.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_loss_log.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Cluster Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_clustering_stats0720.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Degree Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ladder_degree_stats.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualisation generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ladder_synth.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualisation test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tree_test.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/loss_tree.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/loss_log_tree.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Cluster Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/cs_tree.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot Degree Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagramme/ds_tree.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualisation generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tree_synth.png\" width=\"1000\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Paper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different graphs were generated in the paper than in this notebook. Therefore, we do not compare with the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation is definitely expandable, as can be seen from the visualisation. For the tree graphs, insufficient graphs are generated, while for ladder graphs\n",
    "ladder graphs we already achieve good results.\n",
    "One possibility would be to use the more complex variant instead of GraphRNN simple. Furthermore, it would be useful to train longer with better hardware. The training hardware was a Carbon X1 from 2018 with Intel¬Æ Core‚Ñ¢ i7-7500U CPU @ 2.70GHz. We used GraphRNN - simple as the implementation alone was very time consuming and simpler model is more trainable with weaker hardware.\n",
    "for\n",
    "Furthermore, I had to deal with a number of other problems. I could not train with an arbitrary amount of data, because if a model was too overfitted, it tended to stop sending EOS.\n",
    "I also found it contradictory that the paper did not distinguish between generation and training.\n",
    "The paper explained the model with a random process, but did not mention that this was omitted for the training process. I only understood this difference when I looked at the actual implementation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J. You et al, 2018. GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models In: ICML 2018\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
